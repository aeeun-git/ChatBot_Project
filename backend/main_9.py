# app.py (FastAPI ë°±ì—”ë“œ)

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Literal, Optional

from openai import OpenAI
from dotenv import load_dotenv
import os

from text_sql_9 import Text_SQL
from text_embed_9 import TEXT_Embed

# í™˜ê²½ ì„¤ì •
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
client = OpenAI(api_key=OPENAI_API_KEY)

# í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
db = Text_SQL()
embedder = TEXT_Embed()

# FastAPI ì´ˆê¸°í™”
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    user_input: str
    # ì„ íƒ ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼ì„ ì œí•œ
    style: Optional[Literal["ì¹œêµ¬ì²´", "ì¡´ëŒ“ë§", "ë¹„ì¦ˆë‹ˆìŠ¤"]] = "ì¹œêµ¬ì²´"

class ChatResponse(BaseModel):
    response: str
    intent: Optional[str] = None

# ìŠ¤íƒ€ì¼ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë§¤í•‘
STYLE_PROMPTS = {
    "ì¹œêµ¬ì²´":   "ë„ˆëŠ” ë‚˜ì˜ ì¹œêµ¬ì•¼. ë°˜ë§ë¡œ ìœ ì¾Œí•˜ê²Œ ëŒ€ë‹µí•´ì¤˜.",
    "ì¡´ëŒ“ë§":   "ë„ˆëŠ” ë‚˜ì˜ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ê³µì†í•˜ê³  ì •ì¤‘í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜.",
    "ë¹„ì¦ˆë‹ˆìŠ¤": "ë„ˆëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì•¼. ë¹„ì¦ˆë‹ˆìŠ¤ ì–´íˆ¬ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜."
}

# í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
def save_chat_to_file(user_input: str, ai_response: str):
    with open("chat_log.txt", "a", encoding="utf-8") as f:
        f.write(f"ğŸ‘¤ ì‚¬ìš©ì: {user_input}\n")
        f.write(f"ğŸ¤– AI: {ai_response}\n")
        f.write("=" * 40 + "\n")

# /chat ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat", response_model=ChatResponse)
def chat_endpoint(request: ChatRequest):
    try:
        system_prompt = STYLE_PROMPTS.get(request.style, STYLE_PROMPTS["ì¹œêµ¬ì²´"])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user",   "content": request.user_input}
        ]

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.7,
        )
        ai_response = response.choices[0].message.content.strip()
        intent = embedder.classify_intent(ai_response)

        # ì €ì¥ (íŒŒì¼ + DB)
        save_chat_to_file(request.user_input, ai_response)
        db.save_message("user", request.user_input)
        db.save_message("assistant", ai_response)

        return {"response": ai_response, "intent": intent}
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail="ì„œë²„ ì˜¤ë¥˜ ë°œìƒ")

# /history ì—”ë“œí¬ì¸íŠ¸
@app.get("/history")
def get_chat_history():
    try:
        return db.get_all_messages()
    except Exception as e:
        print(f"âŒ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ëŒ€í™” ê¸°ë¡ ì¡°íšŒ ì˜¤ë¥˜")
